# âš¡ GateX

**Your gateway to AI models** â€” Access VS Code LLM providers via a simple HTTP API.

Turn your GitHub Copilot subscription into a local AI API server. Use Claude, GPT-4o, Gemini, and more with any OpenAI or Anthropic SDK.

## âœ¨ Features

- ğŸš€ **Zero Configuration** â€” Starts automatically, default port `24680`
- ğŸ”Œ **Dual API Format** â€” OpenAI + Anthropic compatible endpoints
- ğŸŒŠ **SSE Streaming** â€” Real-time streaming responses
- ğŸ”„ **Smart Retry** â€” Exponential backoff for transient failures
- ğŸ¯ **Multi-Model Support** â€” Access Claude, GPT-4o, Gemini, and more
- âš™ï¸ **IDE Config Generator** â€” One-click setup for Claude Code, .env, etc.

## ğŸš€ Quick Start

### 1. Install & Activate

The extension starts automatically when VS Code opens. Look for the status bar item:

```
âš¡ GateX :24680
```

### 2. Use in Your Code

**OpenAI Format:**

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:24680/v1",
    api_key="gatex"  # Any value works
)

response = client.chat.completions.create(
    model="claude-sonnet-4",  # Or: gpt-4o, gpt-4o-mini, etc.
    messages=[{"role": "user", "content": "Hello!"}],
    stream=True
)

for chunk in response:
    print(chunk.choices[0].delta.content or "", end="")
```

**Anthropic Format:**

```python
from anthropic import Anthropic

client = Anthropic(
    base_url="http://localhost:24680",
    api_key="gatex"
)

message = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Hello!"}],
    stream=True
)

for event in message:
    if event.type == "content_block_delta":
        print(event.delta.text, end="")
```

### 3. Configure Your IDE

Press `Cmd+Shift+P` (or `Ctrl+Shift+P`) â†’ `GateX: Configure for IDE` to auto-generate configs for:

- **Claude Code** â€” Updates `.claude/settings.json` with dual model selection (main + fast), onboarding bypass, and telemetry opt-out
- **Codex CLI** â€” Generates `~/.codex/config.toml` with GateX as model provider
- **.env File** â€” Generates both OpenAI and Anthropic environment variables
- **Clipboard** â€” Copies `export` or `$env:` commands for your terminal

The config generator uses **smart merge** â€” it only updates GateX-related keys, preserving all your other settings. Model picker shows grouped results with â­ recommendations per IDE.

## ğŸ“¡ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/models` | GET | List available models |
| `/v1/chat/completions` | POST | OpenAI format chat |
| `/v1/messages` | POST | Anthropic format chat |
| `/v1/health` | GET | Server health status |

## âš™ï¸ Configuration

| Setting | Default | Description |
|---------|---------|-------------|
| `gatex.port` | `24680` | Server port (0 = auto select) |
| `gatex.timeout` | `300` | Request timeout in seconds |
| `gatex.maxRetries` | `3` | Max retry attempts |
| `gatex.vendors` | `["copilot"]` | Model vendors to expose (`copilot`, `aitk-github`, `aitk-foundry`, `*`) |

## ğŸ¯ Commands

| Command | Description |
|---------|-------------|
| `GateX: Configure for IDE` | Generate config for Claude Code, .env, etc. |
| `GateX: Show Connection Info` | Display endpoint, models, and quick actions |
| `GateX: Copy Endpoint URL` | Copy endpoint to clipboard |
| `GateX: Check Model Health` | Test all models |
| `GateX: Restart Server` | Restart the HTTP server |

## ğŸ”§ Troubleshooting

### No models available?
Make sure you have GitHub Copilot or another LLM extension installed and signed in.

### Port already in use?
Set `gatex.port` to `0` for auto-selection, or choose a different port.

### Request timeout?
Increase `gatex.timeout` in settings for long-running requests.

### Model not found?
GateX uses fuzzy matching â€” try shorter names like `claude-sonnet-4` or `gpt-4o`. Run `GateX: Show Connection Info` to see available models.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Your App / IDE / CLI                            â”‚
â”‚  (OpenAI SDK / Anthropic SDK / curl)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ HTTP (localhost:24680)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GateX Extension                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  server.ts â€” HTTP Router                â”‚     â”‚
â”‚  â”‚  â€¢ /v1/chat/completions (OpenAI)        â”‚     â”‚
â”‚  â”‚  â€¢ /v1/messages (Anthropic)             â”‚     â”‚
â”‚  â”‚  â€¢ Retry with exponential backoff       â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  models.ts â€” Model Manager              â”‚     â”‚
â”‚  â”‚  â€¢ Exact + fuzzy matching               â”‚     â”‚
â”‚  â”‚  â€¢ 30s model cache                      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  configGenerator.ts â€” IDE Setup         â”‚     â”‚
â”‚  â”‚  â€¢ Claude Code (dual model + onboarding)â”‚     â”‚
â”‚  â”‚  â€¢ Codex CLI (TOML config)              â”‚     â”‚
â”‚  â”‚  â€¢ .env, clipboard                      â”‚     â”‚
â”‚  â”‚  â€¢ Smart merge (preserve existing)      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ vscode.lm API (zero auth)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VS Code Language Model Providers                â”‚
â”‚  (GitHub Copilot, etc.)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Source Files

| File | Lines | Description |
|------|-------|-------------|
| `server.ts` | ~490 | HTTP server, OpenAI + Anthropic handlers, retry |
| `configGenerator.ts` | ~460 | IDE config generation (Claude Code, Codex, .env) |
| `models.ts` | ~195 | VS Code LM model management + vendor filter |
| `extension.ts` | ~160 | Entry point, command registration |
| `statusBar.ts` | ~80 | Status bar display |

**Total: ~1,385 lines** â€” lean and focused.

## ğŸ“„ License

MIT

## ğŸ™ Acknowledgments

Built with â¤ï¸ using the VS Code Language Model API.
